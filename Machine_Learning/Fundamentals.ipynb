{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating machine-learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a model always boils down to splitting the available data into three sets:\n",
    "training, validation, and test. You train on the training data and evaluate your model\n",
    "on the validation data(in order to prevent information leaks, you shouldn’t tune your model based on the test set, and therefore you\n",
    "should also reserve a validation set). Once your model is ready for prime time, you test it one final\n",
    "time on the test data. This may seem straightforward,\n",
    "but there are a few advanced ways to do it. Let’s review three classic evaluation recipes:<br> Simple hold-out validation, K-fold validation, and Iterated K-fold validation with shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE HOLD-OUT VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 10000\n",
    "\n",
    "#Shuffling the data is usually appropriate.\n",
    "np.random.shuffle(data)\n",
    "\n",
    "#Defines the validation set\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "#Defines the training set\n",
    "training_data = data[:]\n",
    "\n",
    "#Trains a model on the training data, and evaluates it on \n",
    "#the validation data\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "\n",
    "#Once you’ve tuned your hyperparameters, it’s common to \n",
    "#train your final model from scratch on all non-test data available.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,validation_data]))\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It suffers from one flaw: if little data is\n",
    "available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K- FOLD VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you split your data into K partitions of equal size. For each parti-\n",
    "tion i , train a model on the remaining K – 1 partitions, and evaluate it on partition i .\n",
    "Your final score is then the averages of the K scores obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "\n",
    "for fold in range(k):\n",
    "    # selecting the validition data partition\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                           num_validation_samples * (fold + 1)]\n",
    "    # using the remainder data as training data\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "    data[num_validation_samples * (fold + 1):]\n",
    "    # create a model\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "# average test score\n",
    "validation_score = np.average(validation_scores)\n",
    "# train the final model on all non-test data.\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITERATED K-FOLD VALIDITION\n",
    "It consists of applying K -fold validation multiple times, shuffling\n",
    "the data every time before splitting it K ways. The final score is the average of the\n",
    "scores obtained at each run of K -fold validation. Note that you end up training and\n",
    "evaluating P × K models (where P is the number of iterations you use), which can very\n",
    "expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imortant points\n",
    "You want both your training set and test set to be representative of the data at hand. For instance, if you’re trying to classify images of digits,you usually should randomly shuffle your data before splitting it into training and test sets.<br>\n",
    "f you’re trying to predict the future given the past (for example, tomorrow’s weather, stock movements, and so on), you should not randomly shuffle your data before splitting it. In such situations, you should always make sure all data in your test set is posterior to the data in the training set.\n",
    "<br>Make sure your training set and validation set are disjoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
